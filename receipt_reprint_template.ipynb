{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"6\"> Receipt Reprint Template <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">Summary <font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"4\">This notebook is a template and guideline for reprinting customer receipts. The below code and accompanying word doc template should allow you to recreate virtually any receipt and letter required to be sent to customers. <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">Before reading or adding to this document please save the document to your local computer or another location to avoid tampering with or changing this document. <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '4'>Libraries required for this notebook: <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not all libraries may be required for the reprint process but these should include all libraries\n",
    "#required for analysis and visualization of the data is required\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import pyodbc\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from functools import reduce\n",
    "from matplotlib.pyplot import figure\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '4'>In order to directly query WU datawarehouses and run the SQL query's included in this notbook you will need to connect the notebook with ODBC using the method below. If you do not know your ODBC data source name (the name of the datawarehouse you would like to query) you can search ODBC on your computer which should bring up the ODBC Data Source Administrator which includes all the names of datasources your computer can directly connect to. <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect with ODBC connection to directly query Netezza darawarehouse \n",
    "#If you do not know your DSN name or are having trouble you can query the databases\n",
    "#via Aginity and save the resulting data to a csv which you can then upload to this notebook\n",
    "#via the pandas read_csv method\n",
    "dsn_name = \"DSN = ENTER YOUR COMPUTERS DSN NAME HERE;\"\n",
    "conn = pyodbc.connect(dsn_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '4'>Below is an example of a SQL query used to identify affected customer transactions which may require remedication. The queries MTCN's can then be used as an input for a larger SQL query which will pull all fields required for a receipt reprint. Notice how the formatted (f string) string method is used for this query (and all SQL queries in this notebook), if you are receiving errors when running a SQL query in this notebook which works in another SQL workbench (Aginity) check to ensure your SQL query is saved as a formatted string. <font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All transactions sent from Rite Aid locations between 09-30-2017 and 03-28-2018\n",
    "#require remediation if the transactions have not yet been picked up\n",
    "#below query identifies the current population\n",
    "rite_aid_sql = f'''\n",
    "SELECT tran.MTCN03 || tran.MTCN07 AS MTCN, fin.*  \n",
    "FROM TDB.ETL_TDB.DB100PDLIB_FNTXKP01 fin\n",
    "LEFT  JOIN TDB.ETL_TDB.DB100PDLIB_DBAGTP01 agt ON fin.RECAGTKEY = agt.AGTKEY\n",
    "LEFT  JOIN TDB.ETL_TDB.DW400PDLIB_DWTXKP01 tran ON fin.TXNKEY = tran.TXNKEY\n",
    "WHERE fin.RECISODTE between '2017-09-30' and '2018-03-28' \n",
    "\tand fin.RCONTRYKEY = 222 \n",
    "\tand fin.PCONTRYKEY = 0 \n",
    "\tand fin.ICONTRYKEY != 222\n",
    "\tand UPPER(agt.AGTNME) like '%RITE AID%'\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '4'> The below line will execute your SQL query and save the query to a notebook which can then be further manipulated <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute above SQL statement\n",
    "rite_aid = pd.read_sql_query(rite_aid_sql,conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '4'>The below line will extract the MTCN's from the affected transaction and create a formatted string of MTCN's which can be used in a SQL query. Note if your above query only identifies each transaction by its transaction key (TXNKEY) you will need to add a step to your query to join the TXNKEY with the MTCN in another table. The query used in this notebook to obtain the required customer data for a receipt reprint requires transaction MTCN's as the input rather than transaction key's. <title>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the list of MTCN's into string which can be used in a SQL statement \n",
    "mtcn = [str(i) for i in rite_aid.MTCN]\n",
    "mtcn = \"','\".join(mtcn)\n",
    "mtcn = \"'\" + mtcn + \"'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '4'>The below SQL queries will pull the bulk of the required data needed for a receipt reprint and takes the mtcn variable above as an input near the bottom of the query. If your query is showing an error check and make sure the mtcn variable is being appropriately called. <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract all data needed to create upload file\n",
    "upload_file_dataframe = \"\"\"\n",
    "SELECT \n",
    "   -- If an MTCN is found in Manual Refunds, Auto Refunds, or Cancelled Transactions tables, return TRUE, else FALSE\n",
    "   NVL2( ManualRefunds.MTCN , 'ManualRefund', NVL2( AutoRefunds.MTCN , 'AutoRefund', NVL2( CancelledTransactions.MTCN , 'Cancelled', 'No'))) AS RefundedOrCancelled,\n",
    "   -- Select all fields from the inner sub-query (only tables from TDB)\n",
    "   TdbInfo.*,\n",
    "  \n",
    "   -- Customer Name, various sources, use what looks best\n",
    "   CopsMedallia.SENDER_NAME,\n",
    "   CopsTransactions.SENDNAME,\n",
    "   CopsMedallia.SENDER_FIRST_NAME,\n",
    "   CopsTransactions.SENDERFIRSTNAME,   \n",
    "   CopsMedallia.SENDER_MIDDLE_NAME,\n",
    "   CopsTransactions.SENDERMIDDLENAME,\n",
    "   CopsMedallia.SENDER_MATERNAL_LAST_NAME,\n",
    "   CopsTransactions.SENDERMATERNALLASTNAME,\n",
    "   CopsMedallia.SENDER_PATERNAL_LAST_NAME,\n",
    "   CopsTransactions.SENDERPATERNALLASTNAME,\n",
    "   \n",
    "   -- Email, use what looks best\n",
    "   CopsMedallia.SENDER_EMAIL,\n",
    "   CopsTransactions.SENDEREMAILADDRESS,\n",
    "   \n",
    "   \n",
    "   -- Customer Address\n",
    "   CopsMedallia.SENDER_STRADR1,\n",
    "   CopsMedallia.SENDER_STRADR2,\n",
    "   CopsMedallia.SENDER_CITY,\n",
    "   CopsTransactions.SENDERCITYNAME,\n",
    "   CopsMedallia.SENDER_STATE,\n",
    "   CopsTransactions.SENDERSTATENAME,\n",
    "   CopsMedallia.SENDER_ZIPCDE,\n",
    "   CopsTransactions.SENDERPOSTALCODE,\n",
    "   CopsMedallia.SENDER_CONTRY,\n",
    "   CopsTransactions.SENDERCOUNTRY,\n",
    "   CopsTransactions.SENDCOUNTRYOFRESIDENCE,\n",
    "   CopsTransactions.SENDERTELEPHONE,\n",
    "   \n",
    "-- Misc\n",
    "   CopsTransactions.TRANSACTIONBRANDIDENTIFIER,\n",
    "   CopsMedallia.LEGAL_ENTITY,\n",
    "-- Only populated for US outbound > $15\n",
    "   DfBuffer.TOTTOREC,\n",
    "   DfBuffer.DATEAVAIL\n",
    "   \n",
    "\n",
    "FROM (\n",
    "    SELECT \n",
    "        MTCN03 || MTCN07 AS MTCN,\n",
    "        FinTransactions.TXNKEY,\n",
    "    \n",
    "    -- PRODUCTS, usually recording product == pay product\n",
    "        FinTransactions.RECPRODCT,\n",
    "        Products.PRDDES AS PRODUCT_DESCRIPTION,\n",
    "        Products.BRAND,\n",
    "         \n",
    "    -- Dates/times\n",
    "        FinTransactions.RECISODTE + SendTimes.HOURMINC AS RECISODTE,\n",
    "        FinTransactions.PAYISODTE + PayoutTimes.HOURMINC AS Pay_Date,\n",
    "        -- Do we need RDATE7?\n",
    "        TransactionDetails.RDATE7,\n",
    "    \n",
    "    -- Principal amounts\n",
    "        HistoricalRecordingFacts.RECPRN,\n",
    "        RecordingCurrencies.CURR AS Send_Currency,\n",
    "        PayoutDetails.PAYPRN AS Pay_Amount,\n",
    "        PayoutCurrencies.CURR AS Pay_Currency,\n",
    "        \n",
    "    -- Tax and Fees\n",
    "        HistoricalRecordingFacts.CHGBAS AS Send_Fee,\n",
    "        HistoricalRecordingFacts.CHGBASUSD AS Send_Fee_USD,\n",
    "        HistoricalRecordingFacts.TAXCITY, \n",
    "        HistoricalRecordingFacts.TAXST,\n",
    "        HistoricalRecordingFacts.TAXMUN, \n",
    "        HistoricalRecordingFacts.TAXTTL,\n",
    "    \n",
    "    -- Fx And Dsc\n",
    "        TransactionDetails.RATERECEXC / 100 as RATERECEXC100,\n",
    "        HistoricalRecordingFacts.RECDSC / 100 as RECDSC100,\n",
    "        TransactionDetails.RATEPAYEXC / 100 as RATEPAYEXC100,\n",
    "        PayoutDetails.PAYDSC / 100 as PAYDSC100,\n",
    "        -- Do we use this??\n",
    "        HistoricalRecordingFacts.AMTDSC / 100 as AMTDSC100,\n",
    "        \n",
    "    -- Locations (Agent based not customer address based)    \n",
    "        RecordingAgents.CONTRY AS Origination_Country,\n",
    "        IntendedCountries.CONTRY AS Intended_Country,\n",
    "        PayoutCountries.CONTRY AS Pay_Country,\n",
    "        \n",
    "    -- Sending/Recording Agent info\n",
    "        RecordingAgents.AGTNME AS Recording_Agent_Name,\n",
    "        -- Change \"sending\" to \"recording\" for consistency\n",
    "        RecordingAgents.AGNTID AS Sending_Agent_ID,\n",
    "        RecordingAgents.ACITY AS Sending_Agent_City,\n",
    "        RecordingAgents.NETCONTRY AS Recording_Network_Country,\n",
    "        RecordingAgents.NETAGTNME AS Recording_Network_Name,\n",
    "        RecordingAgents.NAID AS Recording_Network_Id,\n",
    "\n",
    "    -- Paying Agent info   \n",
    "        PayingAgents.AGTNME AS Paying_Agent_Name,\n",
    "        PayingAgents.AGNTID AS Paying_Agent_ID,\n",
    "        PayingAgents.ACITY AS Paying_Agent_City,\n",
    "        PayingAgents.NETCONTRY AS Paying_Network_Country,\n",
    "        PayingAgents.NETAGTNME AS Paying_Network_Name,\n",
    "        PayingAgents.NAID AS Paying_Network_Id,\n",
    "\n",
    "    -- Misc\n",
    "        TransactionDetails.PRMCDE\n",
    "\n",
    "    FROM TDB.ETL_TDB.DB100PDLIB_FNTXKP01 FinTransactions\n",
    "        \n",
    "        INNER JOIN TDB.ETL_TDB.DB100PDLIB_FNPRDP01 Products on FinTransactions.PRDKEY = Products.PRDKEY \n",
    "        INNER JOIN TDB.ETL_TDB.DW400PDLIB_DWTXKP01 TransactionDetails ON FinTransactions.TXNKEY = TransactionDetails.TXNKEY\n",
    "        INNER JOIN TDB.ETL_TDB.DB100PDLIB_DBTMSP01 SendTimes ON FinTransactions.RECTMSKEY = SendTimes.TMSKEY\n",
    "        INNER JOIN TDB.ETL_TDB.DW200PDLIB_DWRTXP01 HistoricalRecordingFacts ON FinTransactions.TXNKEY = HistoricalRecordingFacts.TXNKEY\n",
    "        INNER JOIN TDB.ETL_TDB.DB100PDLIB_DBCURP01 RecordingCurrencies ON FinTransactions.RECCURKEY = RecordingCurrencies.CURKEY\n",
    "        INNER JOIN TDB.ETL_TDB.DB100PDLIB_DBCTYP01 IntendedCountries ON FinTransactions.ICONTRYKEY = IntendedCountries.CONTRYKEY\n",
    "        INNER JOIN TDB.ETL_TDB.DB100PDLIB_DBCTYP01 RecordingCountries ON FinTransactions.RCONTRYKEY = RecordingCountries.CONTRYKEY\n",
    "        \n",
    "        -- Recording agent key\n",
    "        INNER JOIN (\n",
    "            SELECT * FROM TDB.ETL_TDB.DB100PDLIB_DBAGTP01 A\n",
    "            LEFT OUTER JOIN TDB.ETL_TDB.DB100PDLIB_DBNWKP01 B ON A.NWKKEY = B.NWKKEY\n",
    "        ) as RecordingAgents ON FinTransactions.RECAGTKEY = RecordingAgents.AGTKEY\n",
    "        \n",
    "        -- Anything Payout must be left joined, unless you want to exclude anything not yet paid out\n",
    "        LEFT OUTER JOIN TDB.ETL_TDB.DB100PDLIB_DBTMSP01 PayoutTimes ON FinTransactions.PAYTMSKEY = PayoutTimes.TMSKEY\n",
    "        LEFT OUTER JOIN TDB.ETL_TDB.DW300PDLIB_DWPTXP01 PayoutDetails ON FinTransactions.TXNKEY = PayoutDetails.TXNKEY\n",
    "        LEFT OUTER JOIN TDB.ETL_TDB.DB100PDLIB_DBCURP01 PayoutCurrencies ON FinTransactions.PAYCURKEY = PayoutCurrencies.CURKEY\n",
    "        LEFT OUTER JOIN TDB.ETL_TDB.DB100PDLIB_DBCTYP01 PayoutCountries ON FinTransactions.PCONTRYKEY = PayoutCountries.CONTRYKEY\n",
    "        \n",
    "        -- Paying agent key\n",
    "        LEFT OUTER JOIN ( \n",
    "            SELECT * FROM TDB.ETL_TDB.DB100PDLIB_DBAGTP01 A\n",
    "            LEFT OUTER JOIN TDB.ETL_TDB.DB100PDLIB_DBNWKP01 B ON A.NWKKEY = B.NWKKEY\n",
    "        ) as PayingAgents ON FinTransactions.PAYAGTKEY = PayingAgents.AGTKEY\n",
    "\n",
    "        WHERE \n",
    "        FINtransactions.recisodte > '2016-06-01'\n",
    "        AND MTCN in ({mtcn}) --The mtcn variable here is being called from the variable you created previous to this query, make sure you do not change the formatting here or you \n",
    "        \n",
    "\n",
    ") TdbInfo\n",
    "\n",
    "LEFT OUTER JOIN COPS.DBAUSER.TRANSACTIONS CopsTransactions ON TdbInfo.MTCN = CopsTransactions.MTCN AND Substr(TdbInfo.RECISODTE,1,10) = Substr(CopsTransactions.RECORDINGDATETIME,1,10)\n",
    "LEFT OUTER JOIN COPS.DBAUSER.MTCN_PRODUCT_MEDALLIA CopsMedallia ON TdbInfo.TXNKEY = CopsMedallia.TXNKEY\n",
    "LEFT OUTER JOIN COPS.DBAUSER.CFPB_DF_BUFFER DfBuffer on TdbInfo.TXNKEY = DfBuffer.TXNKEY\n",
    "\n",
    "-- These are the (incomplete) refund tables. We use them to indicate refunded txns, but missing values are common.\n",
    "LEFT OUTER JOIN COPS.DBAUSER.MTCN_MANUAL_REFUND ManualRefunds ON TdbInfo.MTCN = ManualRefunds.MTCN AND Substr(TdbInfo.RECISODTE,1,10) = Substr(ManualRefunds.REC_DATETIME,1,10)\n",
    "LEFT OUTER JOIN COPS.DBAUSER.MTCN_AUTO_REFUND AutoRefunds ON TdbInfo.MTCN = AutoRefunds.MTCN AND Substr(TdbInfo.RECISODTE,1,10) = Substr(AutoRefunds.REC_DATETIME,1,10)\n",
    "LEFT OUTER JOIN COPS.DBAUSER.MTCN_CANCEL_TXNS CancelledTransactions ON TdbInfo.MTCN = CancelledTransactions.MTCN AND Substr(TdbInfo.RECISODTE,1,10) = Substr(CancelledTransactions.REC_DATETIME,1,10)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQL query to identify customers who have not picked up transaction with bad receipts and identify customer information\n",
    "#required to mail a complete receipt to the customer\n",
    "#these queries are used to create the customer receipts rather than the upload file\n",
    "receipt_reprint_dataframe = \"\"\"\n",
    "SELECT  riteaid.MTCN, riteaid.TXNKEY, med.SENDER_NAME, riteaid.BUSPRDGRP, riteaid.principal_usd, \n",
    "\t\triteaid.CHGBASUSD, riteaid.AMTDSC, riteaid.AMTGROSS, riteaid.convert_local_currency,\n",
    "\t\triteaid.pay_amount_local_currency,riteaid.RECISODTE, med.RECORDING_DATE, \n",
    "\t\tmed.RECORDING_TIME, med.SENDER_EMAIL, med.SENDER_STRADR1, med.SENDER_CITY, \n",
    "\t\tmed.SENDER_STATE, med.SENDER_CONTRY, med.PAYER_NAME, currency.RECORDING_CURRENCY, \n",
    "\t\tmed.INTENDED_COUNTRY, currency.INTENDED_CURRENCY, med.SENDER_CTYDES, med.SENDER_ZIPCDE, \n",
    "\t\triteaid.AGTKEY, riteaid.AGNTID,riteaid.CSBPHONE1, riteaid.CSBURL1,riteaid.STAGCNME, \n",
    "\t\triteaid.DATEAVAIL, riteaid.AGTNME, riteaid.AADDR1, riteaid.ACITY, riteaid.ASTATE, \n",
    "\t\triteaid.ZIPCDE, riteaid.CONTRY\n",
    "\t\t\n",
    "FROM COPS.DBAUSER.MTCN_PRODUCT_MEDALLIA med\n",
    "INNER JOIN(\n",
    "SELECT fin.TXNKEY, fin.RECAGTKEY, fin.RECISODTE, agents.AGTKEY,prod.BUSPRDGRP, \n",
    "agents.AGNTID, tran.MTCN03||tran.MTCN07 AS MTCN, hist.RCPUSD AS principal_usd, hist.RECDSC, \n",
    "hist.CHGBASUSD, hist.AMTDSC, hist.AMTGROSS, buff.CSBPHONE1, buff.CSBURL1, \n",
    "tran.RATERECEXC/100*tran.RATEPAYEXC/100 AS convert_local_currency,convert_local_currency*hist.AMTGROSS AS pay_amount_local_currency, \n",
    "buff.STAGCNME, buff.DATEAVAIL, agents.AGTNME, agents.AADDR1, agents.ACITY, \n",
    "agents.ASTATE, agents.ZIPCDE, agents.CONTRY\n",
    "FROM TDB.ETL_TDB.DB100PDLIB_FNTXKP01 fin\n",
    "\n",
    "INNER JOIN TDB.ETL_TDB.DB100PDLIB_DBAGTP01 agents ON fin.RECAGTKEY = agents.AGTKEY            \n",
    "INNER JOIN TDB.ETL_TDB.DB100PDLIB_FNPRDP01 prod ON fin.RECPRDKEY = prod.PRDKEY\n",
    "INNER JOIN TDB.ETL_TDB.DW400PDLIB_DWTXKP01 tran ON fin.TXNKEY = tran.TXNKEY\n",
    "INNER JOIN TDB.ETL_TDB.DW200PDLIB_DWRTXP01 hist ON fin.TXNKEY = hist.TXNKEY\n",
    "INNER JOIN COPS.DBAUSER.CFPB_DF_BUFFER buff ON fin.TXNKEY = buff.TXNKEY\n",
    "\n",
    "WHERE fin.RECISODTE BETWEEN '2017-09-30' AND '2018-03-28'\n",
    "    AND fin.RCONTRYKEY = 222\n",
    "    -- Filter out unpaid/cancelled txns and domestic transctions\n",
    "    AND fin.PCONTRYKEY = 0\n",
    "    AND fin.ICONTRYKEY != 222\n",
    "    -- Only Rite Aid recorded transactions\n",
    "    AND UPPER(Agents.AGTNME) LIKE '%RITE AID%') as riteaid ON med.MTCN = riteaid.MTCN\n",
    "\t\n",
    "INNER JOIN(\n",
    "SELECT tran.TXNKEY, icurr.CURR AS INTENDED_CURRENCY, rcurr.CURR AS RECORDING_CURRENCY\n",
    "FROM TDB.ETL_TDB.DB100PDLIB_FNTXKP01 tran\n",
    "LEFT JOIN TDB.ETL_TDB.DB100PDLIB_DBCURP01 icurr ON tran.INTCURKEY = icurr.CURKEY  \n",
    "LEFT JOIN TDB.ETL_TDB.DB100PDLIB_DBCURP01 rcurr ON tran.RECCURKEY = rcurr.CURKEY) as currency ON riteaid.TXNKEY = currency.TXNKEY\n",
    "\n",
    "WHERE med.RECORDING_DATE BETWEEN '2017-06-30' AND '2018-03-28'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two final data points are required to complete the information for the receipt\n",
    "#iCare case number will be obtained after providing upload file and cases are created\n",
    "#for each receipt. Recording operator ID numbers are also required, the below query searches for the \n",
    "#recording operator ID \n",
    "recording_operator_id_sql_query = f\"\"\"\n",
    "SELECT * FROM COPS.DBAUSER.MTCN_ACTIVITY act\n",
    "WHERE act.ACTTYP = 'REC' \n",
    "AND act.ACTDTE > '2016-12-01'\n",
    "AND MTCN in ({mtcn})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '4'>The below line will execute the above SQL query. After executing the query it is recomended you export the data frame to either a pickle or csv file on your computer in order to easily and quickly reload the data incase you have to restart this notebooks kernel and clear all outputs as the above query can take 10 - 15 minutes to execute.<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute above SQL statements\n",
    "#If the raw_data_query has already been done for the day, export it \n",
    "#to either a pickle or csv and import via the read_pickle\n",
    "#or read_csv pandas function to save time\n",
    "upload_file_data = pd.read_sql_query(upload_file_data,conn)\n",
    "receipt_reprint_data = pd.read_sql_query(receipt_reprint_dataframe,conn)\n",
    "operator_id_data = pd.read_sql_query(recording_operator_id_sql_query,conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '4'> Below are two methods to read in pickle or csv files which is quicker than running the above queries. This method is recomended if the above queries have already been run and saved earlier or if their were issues connecting to the ODBC driver and the SQL queries were run and exported from an alternative SQL workbench. The # before the lines indicates they are currently read as comments and will not execute, in order to execute the lines remove the # and add the file paths. <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload_file_data = pd.read_csv(r'exported file path', sep = '|')\n",
    "#upload_file_data = pd.read_pickle](path_to_pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop duplicate MTCN's, the first recording operator id\n",
    "#will be kept as that was the operator who first touched\n",
    "#the transaction\n",
    "recording_operator_id = recording_operator_id.drop_duplicates(subset = \"MTCN\", keep = 'first')\n",
    "receipt_reprint_data = receipt_data.drop_duplicates(subset = 'TXNKEY')\n",
    "upload_file_data = recording_operator_id.drop_duplicates(subset = \"MTCN\", keep = 'first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '4'> The recording operator id data frame will be merged with the receipt reprint data in order to create a single unified file which includes all information needed to create a new customer receipt (with the exception of he iCare case number which can only be obtained after uploading the upload file to Pune and case numbers are generated). <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge receipt_data and recording_operator_id\n",
    "receipt_receording_merged = pd.merge(receipt_data, recording_operator_id, how = 'left', left_on = 'MTCN', right_on = 'MTCN' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '4'>The below lines will create the mail_merge data frame which will ultimately be exported and used to create the customer receipts. If additional fields are needed they can be added below but you may need to adjust the SQL queries to include the additional inforation. <title>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe containing only information required for mail merge\n",
    "mail_merge = pd.DataFrame()\n",
    "mail_merge['MTCN'] = receipt_receording_merged.MTCN\n",
    "mail_merge['operator_id'] = receipt_receording_merged.OPRID\n",
    "mail_merge['product'] = receipt_receording_merged.BUSPRDGRP\n",
    "mail_merge['agent_name'] = receipt_receording_merged.AGTNME\n",
    "mail_merge['agent_address'] = receipt_receording_merged.AADDR1\n",
    "mail_merge['agent_city'] = receipt_receording_merged.ACITY\n",
    "mail_merge['agent_state'] = receipt_receording_merged.ASTATE\n",
    "mail_merge['agent_zip_code'] = receipt_receording_merged.ZIPCDE\n",
    "mail_merge['transaction_date'] = pd.DatetimeIndex(receipt_receording_merged.RECISODTE).strftime('%B%d%Y')\n",
    "mail_merge['customer_name'] = receipt_receording_merged.SENDER_NAME\n",
    "mail_merge['time_of_transaction'] = pd.DatetimeIndex(receipt_receording_merged.RECORDING_TIME).strftime(\"%I:%M %p\")\n",
    "mail_merge['customer_name'] = receipt_receording_merged.SENDER_NAME\n",
    "mail_merge['transaction_principal'] = round(receipt_receording_merged.PRINCIPAL_USD,2).apply(lambda row: str(row))\n",
    "#mail_merge['transaction_principal'] = round(receipt_data.PRINCIPAL_USD.apply(lambda row: str(round(row,2)))                                            \n",
    "mail_merge['transaction_fee'] = round(receipt_receording_merged.CHGBASUSD,2).apply(lambda row: str(row))\n",
    "mail_merge['transaction_total'] = round(receipt_receording_merged.AMTGROSS,2).apply(lambda row: str(row))\n",
    "mail_merge['exchange_rate'] = round(receipt_receording_merged.CONVERT_LOCAL_CURRENCY,2).apply(lambda row: str(row))\n",
    "mail_merge['int_currency'] = receipt_receording_merged.INTENDED_CURRENCY\n",
    "mail_merge['transaction_date'] = pd.DatetimeIndex(receipt_receording_merged.RECISODTE).strftime('%m/%d/%Y')\n",
    "mail_merge['customer_name'] = receipt_receording_merged.SENDER_NAME\n",
    "mail_merge['customer_adress'] = receipt_receording_merged.SENDER_STRADR1.apply(lambda row:row.title())\n",
    "mail_merge['customer_city'] = receipt_receording_merged.SENDER_CITY.apply(lambda row:row.title())\n",
    "mail_merge['customer_state'] = receipt_receording_merged.SENDER_STATE\n",
    "mail_merge['customer_zip'] = receipt_receording_merged.SENDER_ZIPCDE\n",
    "mail_merge['sender_country'] = receipt_receording_merged.SENDER_CTYDES\n",
    "mail_merge['receiver_country'] = receipt_receording_merged.INTENDED_COUNTRY\n",
    "mail_merge['receiver_name'] = receipt_receording_merged.PAYER_NAME\n",
    "mail_merge['date_of_availability'] = receipt_receording_merged.DATEAVAIL.apply(lambda row: datetime.datetime(int(row[-4:]),int(row[:2]),int(row[2:4])).strftime('%B %d, %Y'))\n",
    "mail_merge['state_financial_regulatory_body'] = receipt_receording_merged.STAGCNME\n",
    "mail_merge['state_financial_regulatory_body_phone'] = receipt_receording_merged.CSBPHONE1\n",
    "mail_merge['state_financial_regulatory_body_website'] = receipt_receording_merged.CSBURL1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '4'> This will export your mail_merge data frame as an excel file which will be required to perform a mail merge with the word template which accompanies this notebook. However you may export the data to a csv and use the data import function in an empty Excel file to import the data in to an excel file, this may allow greater customization of the settings of the data within the excel file (basically you can change excel column data types when importing a csv where as a direct excel export may result in certain columns having the incorrect data types). You may also receive a series of errors  about exceeding Excel's limit for URLS, these errors can be ignored and are caused by embedded links in the data frame.<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the mail_merge dataframe to an Excel doc to complete mail merge\n",
    "writer = pd.ExcelWriter(r'file path.xlsx')\n",
    "mail_merge.to_excel(writer, index = False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '4'>The remainder of this notebook covers the creation of an upload file which needs to be created and uploaded into iCare in order to create iCare case numbers which will be included in the letter which will accompany the receipt sent to customers. In order for the case numbers to be generated the upload file needs to be formatted as specified below and sent to Pune. <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy raw data for manipulation into upload file\n",
    "upload_template_df = upload_file_data.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_template_df = upload_template_df.where((pd.notnull(upload_template_df)), None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '4'> The below functions are required to format and merge required data for the upload file. <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the open_transactions DF to an upload file\n",
    "#Format first, middle and last name of sender\n",
    "\n",
    "def fallback(row, col1, col2):\n",
    "    if row[col1] is not None:\n",
    "        return row[col1]\n",
    "    elif row[col2] is not None:\n",
    "        return row[col2]\n",
    "    else:\n",
    "        # always return at least an empty string\n",
    "        return ''\n",
    "\n",
    "def first_name(row):\n",
    "    return fallback(row, 'SENDERFIRSTNAME','SENDER_FIRST_NAME')\n",
    "    \n",
    "def middle_name(row):\n",
    "    return fallback(row, 'SENDERMIDDLENAME', 'SENDER_MIDDLE_NAME')\n",
    "\n",
    "def maternal_name(row):\n",
    "    return fallback(row, 'SENDERMATERNALLASTNAME','SENDER_MATERNAL_LAST_NAME')\n",
    "\n",
    "def paternal_name(row):\n",
    "    return fallback(row, 'SENDERPATERNALLASTNAME','SENDER_PATERNAL_LAST_NAME')\n",
    "\n",
    "def last_name(row):\n",
    "    last_name = paternal_name(row) + ' ' + maternal_name(row)\n",
    "    return last_name.strip()\n",
    "\n",
    "#Check if upload_email field has an email in it, if so return 'Yes' otherwise leave blank\n",
    "def email(row):\n",
    "    return fallback(row, 'SENDER_EMAIL', 'SENDEREMAILADDRESS')\n",
    "\n",
    "#Identify customer emails if required (only digital US customers and non-US customers will have their emails retained)\n",
    "def should_send_email(row):\n",
    "    email_address = email(row)\n",
    "    if email_address == '':\n",
    "        return '' # dont send email\n",
    "    elif row['SENDER_CONTRY'] !='US':\n",
    "        return 'Yes' # send email\n",
    "    elif row['RECPRODCT'][0] == 'F':\n",
    "        return 'Yes'\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "#Check both customer street fields and merge if both fields contain information\n",
    "def street_merge(row):\n",
    "    if row['SENDER_STRADR1'] == None or 'NaN':\n",
    "        return ''\n",
    "    else:\n",
    "        return row['SENDER_STRADR1'].strip()\n",
    "\n",
    "def city_check(row):\n",
    "    return fallback(row,'SENDER_CITY','SENDERCITYNAME')\n",
    "\n",
    "def state_check(row):\n",
    "    return fallback(row,'SENDER_STATE', \"SENDERSTATENAME\")\n",
    "\n",
    "def zipcode_check(row):\n",
    "    return fallback(row, 'SENDERPOSTALCODE', 'SENDER_ZIPCDE')\n",
    "\n",
    "def country_check(row):\n",
    "    return fallback(row, 'SENDER_CONTRY', 'SENDCOUNTRYOFRESIDENCE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '4'> The below will format your upload_templat_df and execute the above functions, certain lines/functions may need to be adjusted based on the information available. <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute above functions to prepare fields for upload file\n",
    "upload_template_df['upload_zipcode'] = upload_template_df.apply(zipcode_check, axis = 1)\n",
    "upload_template_df['upload_street'] = upload_template_df.apply(street_merge, axis = 1)\n",
    "upload_template_df['upload_city'] = upload_template_df.apply(city_check, axis = 1)\n",
    "upload_template_df['upload_state'] = upload_template_df.apply(state_check, axis = 1)\n",
    "upload_template_df['sender_first_name_updated'] = upload_template_df.apply(first_name,axis = 1)\n",
    "upload_template_df['sender_middle_name_updated'] = upload_template_df.apply(middle_name,axis = 1)\n",
    "upload_template_df['sender_last_name_updated'] = upload_template_df.apply(last_name,axis = 1)\n",
    "upload_template_df['upload_email'] = upload_template_df.apply(email, axis = 1)\n",
    "upload_template_df['upload_zipcode'] = upload_template_df.apply(zipcode_check, axis = 1)\n",
    "upload_template_df['upload_country'] = upload_template_df.apply(country_check, axis = 1)\n",
    "upload_template_df['long_mtcn'] = upload_template_df.apply(lambda row: pd.to_datetime(row['RECISODTE'],errors = 'coerce').strftime('%y%j') + '8' + str(row.MTCN), axis = 1)\n",
    "upload_template_df['Refund Amount'] = upload_template_df.SEND_FEE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '4'> The country names in iCare need to be formatted in a specific manner which does not align with the formatting in our databases. The below lines will read in a table with the appropriate iCare country formatting which will then be merged with the upload file to ensure proper formatting for the Originiation Country, Intended Country and Pay Country fields. <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import iCare Country lookup codes to convert current country codes to names accepted by iCare\n",
    "iCare_cc = pd.read_csv(r\"C:\\Users\\309270\\Desktop\\portfolio_analytics\\templates\\iCare-Country-lookup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace country codes in final upload report with iCare country names\n",
    "upload_template_df = pd.merge(upload_template_df, iCare_cc, how = 'left', left_on = 'ORIGINATION_COUNTRY', right_on = 'CONTRY')\n",
    "upload_template_df = pd.merge(upload_template_df, iCare_cc, how = 'left', left_on = 'INTENDED_COUNTRY', right_on = 'CONTRY')\n",
    "upload_template_df = pd.merge(upload_template_df, iCare_cc, how = 'left', left_on = 'PAY_COUNTRY', right_on = 'CONTRY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '4'> In order to automatically assign bots to take the uploaded cases, the bots need to be added to the upload_template_df. Before running this line check the available bot id's and make sure they match the below id's otherwise the bots will not pickup the transactions. The below list of bots should be updated/changed based on the available bots. <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list available bot ids\n",
    "remediation_bots = [ '005150000077DHh', '005150000077DIe', '005150000077DIP', '0051C000005QbW2', '0051C000005QbWH', '0051C000005QbWM' ]\n",
    "# just making sure the list is at least as long as the df...\n",
    "remediation_bots_iter = iter(remediation_bots * len(upload_template_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '4'> The below lines will finalze the upload_file and ensure appropriate formatting. Specific fields will need to be adjusted based on the specific cases:\n",
    "    * The Description, REM Letter Resolution and Public Comment fields will need to be updated as they change with each case\n",
    "    * Fields such as the Pay Date or Pay Amount may also need to be adjusted as they vary based on each case \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create final upload file\n",
    "upload_file = pd.DataFrame()\n",
    "upload_file['Case Owner'] = [next(remediation_bots_iter) for i in upload_template_df.index]\n",
    "upload_file['Subject'] = ['ADD CASE NUMBER AND NAME HERE']*len(upload_template_df)\n",
    "upload_file['Description'] = ['']*len(upload_template_df) #Description field is to be left empty, issue description can be found in the 'REM Letter Resolution Text' field\n",
    "upload_file['Status'] = ['New']*len(upload_template_df)\n",
    "upload_file['Case Record Type'] = ['Remediation']*len(upload_template_df)\n",
    "upload_file['REM Letter Resolution Text'] = ['ADD REM LETTER RESOLUTION LANGUAGE HERE']*len(upload_template_df)\n",
    "upload_file['Preferred Language'] = ['']*len(upload_template_df) #Preferred language is to be left empyt\n",
    "upload_file['Refund Amount'] = upload_template_df.apply(lambda row: round(row['Refund Amount'],2),axis = 1)\n",
    "upload_file['Refund Currency'] = upload_template_df['SEND_CURRENCY']\n",
    "upload_file['Remediation'] = ['Refund Fee']*len(upload_template_df)\n",
    "upload_file['Public Comments'] = ['ADD PUBLIC COMMENTS HERE']*len(upload_template_df) #Manually entered information relating to the reason for the refund\n",
    "upload_file['REM First Name'] = upload_template_df['sender_first_name_updated']\n",
    "upload_file['REM Middle Name'] = upload_template_df['sender_middle_name_updated']\n",
    "upload_file['REM Last Name'] = upload_template_df['sender_last_name_updated']\n",
    "upload_file['REM Phone Number'] = upload_template_df['SENDERTELEPHONE']\n",
    "upload_file['REM Email'] = upload_template_df['upload_email']\n",
    "upload_file['Street'] = upload_template_df['upload_street']\n",
    "upload_file['City'] = upload_template_df['upload_city']\n",
    "upload_file['State'] = upload_template_df['upload_state']\n",
    "upload_file['Zip/Postal Code'] = upload_template_df['upload_zipcode']\n",
    "upload_file['Country'] = upload_template_df['CTYDES_x']\n",
    "upload_file['Long MTCN'] = upload_template_df['long_mtcn']\n",
    "upload_file['MTCN'] = upload_template_df['MTCN'] \n",
    "upload_file['Send Date'] = pd.DatetimeIndex(upload_template_df['RECISODTE']).strftime('%m/%d/%Y')\n",
    "upload_file['Send Amount'] = upload_template_df.apply(lambda row: round(row['RECPRN'],2), axis = 1)\n",
    "upload_file['Send Currency'] = upload_template_df['SEND_CURRENCY']\n",
    "upload_file['Send Fee'] = upload_template_df['SEND_FEE']\n",
    "upload_file['Origination Country'] = upload_template_df['CTYDES_x']\n",
    "upload_file['Send Brand'] = upload_template_df['BRAND'] \n",
    "upload_file['Pay Date'] = \"\" #None of the transactions have been picked up but the bots may have a problem interpreting a Nan field\n",
    "#upload_file['Pay Date'] = pd.DatetimeIndex(upload_template_df['PAY_DATE']).strftime('%m/%d/%Y')#Add logic to find any 0 values and change to Null value\n",
    "upload_file['Pay Amount'] = 0 #Only individuals who have not picked up the transaction are eligible to receive receipts\n",
    "upload_file['Pay Currency'] = upload_template_df['PAY_CURRENCY']  \n",
    "upload_file['Destination Country'] = upload_template_df['CTYDES_y']\n",
    "upload_file['Sending Agent ID'] = upload_template_df['SENDING_AGENT_ID']\n",
    "upload_file['Send Email'] = upload_template_df.apply(should_send_email, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '4'> When exporting a csv file make sure the file ends as a .txt rather than .csv, they are the same but the Pune team may push back if not formatted with a .txt file. Also ensure the sep is set to pipe deliminated which is '|' and the index is set to 'False' which  removes the index column from the exported file. <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_file.to_csv(r'C:\\Users\\309270\\Desktop\\portfolio_analytics\\rite_aid_receipt_issue\\upload_file.txt', sep = '|',index = 'False')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '4'> Afer uploadin the upload file, review it to ensure there are no errors, in particular look at the maximum refund amount to ensure there were no errors which would result in over refunding customers. After the document is reviewed send the upload file to the Technology Engineering Center who will then upload the file into iCare and generate case numbers for each refund. <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '4'> After uploading the file the case numbers can be queried from the iCare database and added to the customer receipt dataframe. After adding this field you will use a mail merge to add the information in the exported excel receipt file to the Word template. If you have not performed a mail merge before search the name in the Word doc's quetsion field which will then provide a step by step guide to using a mail merge. <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '5'>Change Log<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '4'>Feb 1, 2019 -- document created by David Thomas <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
